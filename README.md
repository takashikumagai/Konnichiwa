# Konnichiwa

Homework submission - files to test/deploy the application

## Overview

### Directory Structure

```
.github   # task 3: CI/CD (GitHub Action)
app       # provided application code
aws       # task 2: Terraform for deploying the application to AWS
loadtest  # load testing to check SLOs
monitor   # task 3: monitoring script (Python and Shell script)
```

### Initial Setup

Please make sure that .env file is present at repository root, with API_KEY defined in it:
```
API_KEY=konnichiwa-api-key_NOT_FOR_PRODUCTION
```

then install uv on your system (used by load test and monitoring script):
```
curl -LsSf https://astral.sh/uv/install.sh | sh
```


## Description

### 1. Containerization

Build the Docker image:

```
cd app
docker build --rm -t konnichiwa .
```

Run the application on a container locally

```
docker run --rm -p 4000:80 --env-file ../.env -t konnichiwa
```

Notes:
- Added gunicorn with `poetry add gunicorn` for better concurrency.
- About Dockerfile
  - creates a non-privileged service user for running the application
  - I really should not have hardcoded the gunicorn parameters in CMD, but making these configurable was not easy. I definitely need to fix this in the future.


### 2. Cloud Deployment

1. Install Terraform
2. Create resources on AWS:
```
cd aws
terraform init
terraform plan
terraform apply
```

Part of the the Terraform code would not work as it attempts to deploy the app over https using the domain I have, i.e. terraform code is tied to my AWS account.
More on this later.


#### Load testing

You can run the load test locally against the locally deployed application:

First, install the dependencies (you only need to do this once)

```
uv sync
```

Then run the application locally on a container (the same `docker run` command described in the previous step):
```
docker run --rm -p 4000:80 --env-file ../.env -t konnichiwa
```

```
uv run --env-file ../.env locust -f locustfile.py --host http://localhost:4000
```

Then go to `http://0.0.0.0:8089` on the browser to access Locust GUI.
Set
- Number of users: 500
- Ramp up: 50

and check the following columns on the bottom row (Aggregated) to see if SLOs are met:

- 95%ile (ms)
- Current RPS


#### AWS deployment notes

The task #2 ended up being the most incomplete part of the assignment submission.
I did confirm that [an image is built successfully and uploaded to ECR](img/ecr-image.png), and load testing works locally at least, but I couldn't load test for the SLOs on AWS.


### 3. CI/CD Pipeline

**Add the following 3 secrets to GitHub Secrets**
1. AWS_ACCESS_KEY_ID
2. AWS_SECRET_ACCESS_KEY
3. API_KEY

The values of 1 and 2 depend on you AWS account, while 3 can be generated by executing the following command from Linux shell:

```
python -c "import secrets, string; print(''.join(secrets.choice(string.ascii_letters + string.digits) for _ in range(40)))"
```


GitHub workflow file `.github/deploy.yml` executes the following steps
and deploys the application with AWS ECS

0. Run tests
1. Check out the source code
2. Configure AWS credentials
3. Copy the key from GitHub secrets to AWS Secrets Manager
4. Log in to AWS ECR
5. Build, tag, and push image to AWS ECR
6. Update the ECS task definition JSON file with the image ID built in step 5
7. Deploy AWS ECS task definition


### 4. Monitoring Script

How to run monitoring script locally

#### Python:

Install dependencies (required only once)
```
cd monitor
uv sync
```

Run the monitoring script
```
cd monitor
uv run --env-file ../.env python monitor.py
```

#### Shell script
```
cd monitor
./monitor.sh
```

I wrote scripts in Python and Shell, to see which is easier to write.
I found it's easier to write a script in Python; there isn't much difference until when we start adding error handling code to cover all the error cases. Doing it with Bash shell would be possible but takes more effort.

### Notes

#### Deploying the application to AWS

There are several ways to deploy the app to AWS:

1. EC2 + manual Docker installation
2. ECS + ECR (EC2 launch type)
3. ECS + ECR (Fargate launch type)
4. EKS

I chose **2** for this assignment because

- #1 is old-fashioned and comes with lots of maintenance.
- #3 is good but I was not sure is Fargates is covered by free tier.
- #4 has more advanced scaling options but felt a bit overkill for this project.


#### Securing AWS credentials API KEY

The following 2 types of storages are used in this project to safeguard the secrets.
These are generally considered secure for storing secrets in production.

- GitHub Secrets
- AWS Secrets Manager

With API_KEY, GitHub Secrets is used as a 'single source of truth'; the key value is uploaded to AWS
every time CI/CD is triggered so that an update to the key will not break the system. 

#### Best practices on secrets management

- Do NOT bake secrets in docker images, as anyone with access to the image will be able to read it
- In other words, pass the secret value from the aforementioned secure storage to the container at the container run phase.

